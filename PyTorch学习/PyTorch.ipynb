{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.538335Z",
     "start_time": "2019-01-23T19:31:28.254112Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch源码\n",
    "Pytorch源码:https://github.com/pytorch/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scalars\n",
    "lower-case letters: $x\\in R$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.549018Z",
     "start_time": "2019-01-23T19:31:28.540276Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uninitialized\n",
    "torch.empty(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.555183Z",
     "start_time": "2019-01-23T19:31:28.551025Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7591])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly initialized\n",
    "torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.560657Z",
     "start_time": "2019-01-23T19:31:28.556649Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.566579Z",
     "start_time": "2019-01-23T19:31:28.562052Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.572777Z",
     "start_time": "2019-01-23T19:31:28.568271Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.3370)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1.337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.578668Z",
     "start_time": "2019-01-23T19:31:28.574417Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map to Python built-in type(内置类型,例如int,char,float,double)\n",
    "torch.tensor(1.5).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.586168Z",
     "start_time": "2019-01-23T19:31:28.582094Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor(1.3)\n",
    "y=torch.tensor(2.7)\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.593106Z",
     "start_time": "2019-01-23T19:31:28.589209Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.4000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x-y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.598906Z",
     "start_time": "2019-01-23T19:31:28.594555Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4815)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.604717Z",
     "start_time": "2019-01-23T19:31:28.600416Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.610127Z",
     "start_time": "2019-01-23T19:31:28.606184Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0307)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponentiation\n",
    "x**y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Vectors\n",
    "lower-case boldface letters: $\\boldsymbol{x}\\in \\mathbb R^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.615956Z",
     "start_time": "2019-01-23T19:31:28.611616Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 7, 9])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.620929Z",
     "start_time": "2019-01-23T19:31:28.617456Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.625757Z",
     "start_time": "2019-01-23T19:31:28.622354Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.630781Z",
     "start_time": "2019-01-23T19:31:28.627282Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 10, 18])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.636022Z",
     "start_time": "2019-01-23T19:31:28.632787Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,  32, 729])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x**y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.641073Z",
     "start_time": "2019-01-23T19:31:28.637502Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.646908Z",
     "start_time": "2019-01-23T19:31:28.642756Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4,  5,  6],\n",
       "        [ 8, 10, 12],\n",
       "        [12, 15, 18]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector outer product\n",
    "torch.ger(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Matrices\n",
    "upper-case boldface letters: $\\boldsymbol{X}\\in \\mathbb R^{m\\times n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.653263Z",
     "start_time": "2019-01-23T19:31:28.648310Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8, 10, 12],\n",
       "        [ 5,  7,  9]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y = torch.tensor([[7, 8, 9], [1, 2, 3]])\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.658141Z",
     "start_time": "2019-01-23T19:31:28.654886Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrices' dim is always 2\n",
    "x.dim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.663273Z",
     "start_time": "2019-01-23T19:31:28.659702Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.668182Z",
     "start_time": "2019-01-23T19:31:28.664628Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.673468Z",
     "start_time": "2019-01-23T19:31:28.669724Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7, 16, 27],\n",
       "        [ 4, 10, 18]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.678250Z",
     "start_time": "2019-01-23T19:31:28.674834Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 4],\n",
       "        [2, 5],\n",
       "        [3, 6]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.683524Z",
     "start_time": "2019-01-23T19:31:28.679669Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 50,  14],\n",
       "        [122,  32]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mm(y.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors(附data_types)\n",
    "upper-case Euler script letters: $\\mathscr{X}\\in \\mathbb{R}^{a_1 \\times \\cdots \\times a_n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.689776Z",
     "start_time": "2019-01-23T19:31:28.684938Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8257, 0.1172, 0.8344, 0.4760],\n",
       "         [0.0617, 0.5030, 0.8404, 0.8235]],\n",
       "\n",
       "        [[0.1758, 0.4977, 0.4014, 0.7472],\n",
       "         [0.8146, 0.0580, 0.2153, 0.2999]],\n",
       "\n",
       "        [[0.9888, 0.5547, 0.1264, 0.8322],\n",
       "         [0.5742, 0.6390, 0.6083, 0.1054]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 layers, each layer the dim is (2*4)\n",
    "torch.rand(3,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.695254Z",
     "start_time": "2019-01-23T19:31:28.691133Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 2.5244e-29, 0.0000e+00],\n",
       "        [2.5244e-29, 5.6052e-45, 0.0000e+00]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uninitialized\n",
    "torch.empty(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.700010Z",
     "start_time": "2019-01-23T19:31:28.696515Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.705484Z",
     "start_time": "2019-01-23T19:31:28.701294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.711072Z",
     "start_time": "2019-01-23T19:31:28.707173Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identity tensor\n",
    "torch.eye(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.716691Z",
     "start_time": "2019-01-23T19:31:28.712483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7., 7., 7.],\n",
       "        [7., 7., 7.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full((2,3),7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.721946Z",
     "start_time": "2019-01-23T19:31:28.718088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5239, 0.2892, 0.9076],\n",
       "        [0.9661, 0.8985, 0.0231]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly initialized from [0,1]\n",
    "torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.727354Z",
     "start_time": "2019-01-23T19:31:28.723259Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2375, -0.3195, -0.4513],\n",
       "        [-1.2769,  2.3078,  0.1242]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly initialized from N(0,1)\n",
    "torch.randn(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.733158Z",
     "start_time": "2019-01-23T19:31:28.728907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 0, 2],\n",
       "        [0, 8, 5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the low can be reach, but the high cannot\n",
    "torch.randint(low=0,high=10,size=(2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.738305Z",
     "start_time": "2019-01-23T19:31:28.734835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 2, 0, 1, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random permutation of ints from 0 to n-1\n",
    "torch.randperm(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Numpy Bridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Numpy to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.785075Z",
     "start_time": "2019-01-23T19:31:28.780715Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "n = np.ones(5)\n",
    "torch.from_numpy(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.791543Z",
     "start_time": "2019-01-23T19:31:28.787001Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.ones(5)\n",
    "x=torch.from_numpy(n)\n",
    "# still point to the same memory!\n",
    "np.add(n, 1, out=n)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## PyTorch to Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.856695Z",
     "start_time": "2019-01-23T19:31:28.852819Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5)\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.862504Z",
     "start_time": "2019-01-23T19:31:28.858491Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(5)\n",
    "n = x.numpy()\n",
    "# still pointing to the same memory!\n",
    "x.add_(1)\n",
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Basic Operations\n",
    "As with scalars, vectors and matrices, we can perform element-wise operations:  +  -  *  **  /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:28.941729Z",
     "start_time": "2019-01-23T19:31:28.936880Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.6540, 0.8720],\n",
       "         [0.7464, 0.6264],\n",
       "         [0.9001, 1.2874]],\n",
       "\n",
       "        [[1.2638, 1.6145],\n",
       "         [0.6562, 0.8704],\n",
       "         [0.4915, 1.3747]]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3,2)\n",
    "y = torch.rand(2,3,2)\n",
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Many operations have aliases(同名物) and in-place(原地) equivalents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.030791Z",
     "start_time": "2019-01-23T19:31:29.026618Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.add(x,y)\n",
    "# in-place: no extra memory allocation\n",
    "x.add_(y)\n",
    "# equal up to predefined tolerance\n",
    "x.allclose(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Contraction(张量收缩)\n",
    "Generalization of vector-vector, matrix-vector, matrix-matrix product etc.<br>\n",
    "$\\mathscr{X}\\in \\mathbb R^{a_1 \\times a_2 \\times\\cdots\\times a_{n-1} \\times a_n}$<br>\n",
    "$\\mathscr{Y}\\in \\mathbb R^{a_n \\times a_{n+1} \\times\\cdots\\times a_{n+m}}$<br>\n",
    "$\\mathscr{Z} = \\mathscr{X} \\times \\mathscr{Y}\\in \\mathbb R^{a_1 \\times a_2\\times\\cdots a_{n-1} \\times a_{n+1}\\times \\cdots\\times a_{n+m}}$<br>\n",
    "For high-oder tensors @ is a batch-matrix multiplication (see 'einsum' for actual tensor contraction)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.113406Z",
     "start_time": "2019-01-23T19:31:29.108989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6339, 0.3071])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,3)\n",
    "y = torch.rand(3)\n",
    "x @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.120938Z",
     "start_time": "2019-01-23T19:31:29.115429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1378, 0.5528, 0.9696],\n",
       "        [1.1279, 1.0920, 1.1578]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2,4)\n",
    "y = torch.rand(4,3)\n",
    "x @ y\n",
    "# torch.mv(x,y) can calculate the case that x = torch.rand(2,4), y =torch.rand(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.127348Z",
     "start_time": "2019-01-23T19:31:29.122421Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0894, 1.0698],\n",
       "         [0.5501, 0.5071],\n",
       "         [1.1137, 1.1044]],\n",
       "\n",
       "        [[1.6371, 1.5839],\n",
       "         [0.8233, 1.1714],\n",
       "         [1.3226, 1.6934]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 3, 4)\n",
    "y = torch.rand(4, 2)\n",
    "x @ y\n",
    "# 可以看出,x的2层2维矩阵和y的二维矩阵分别作了矩阵乘法，所以张量乘法事实上还是转化成了矩阵乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.176767Z",
     "start_time": "2019-01-23T19:31:29.172971Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 6).view(2, 3)\n",
    "# arange是按行先排列的\n",
    "x.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.182077Z",
     "start_time": "2019-01-23T19:31:29.178549Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in low version, there is some mistake about allclose funtion\n",
    "x.t().allclose(x.transpose(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.187907Z",
     "start_time": "2019-01-23T19:31:29.183622Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(0, 1).allclose(x.transpose(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.193616Z",
     "start_time": "2019-01-23T19:31:29.189855Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0, 12).view(2, 3, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.198886Z",
     "start_time": "2019-01-23T19:31:29.195012Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  2,  4],\n",
       "         [ 1,  3,  5]],\n",
       "\n",
       "        [[ 6,  8, 10],\n",
       "         [ 7,  9, 11]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose(1,2)\n",
    "# 两层不动，每一层代表的矩阵转置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum, Min, Max, and Mean\n",
    "Important: 关于那一维度操作,则是对那一维度变化，其他维度不变化的元素进行操作,求完之后进行squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.245451Z",
     "start_time": "2019-01-23T19:31:29.241370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,12).view(2,3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.251140Z",
     "start_time": "2019-01-23T19:31:29.247678Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(66)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.257296Z",
     "start_time": "2019-01-23T19:31:29.252763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  5,  9],\n",
       "        [13, 17, 21]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sum(2)\n",
    "# 按照第二维度求和，即按第二维度变动的元素求和，即按列求和(第0维为高,第一维为行,第二维为列)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.262954Z",
     "start_time": "2019-01-23T19:31:29.259645Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.268188Z",
     "start_time": "2019-01-23T19:31:29.264404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [6, 7]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min(0),对应在每一层中找最小的,但min(1)表示每一列中找最小的，同时min(2)表示在每一行中找最小的\n",
    "values, indices = x.min(1)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-23T19:31:29.273227Z",
     "start_time": "2019-01-23T19:31:29.269464Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices(索引)\n",
    "# min(0)对应indices(索引)表示的在每一层找的最小值的位置，min(1),min(2)以此类推\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, indices = x.max(0)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.5000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean is not defined for Long tensors, so if you use x=x.long(), there will be a \n",
    "x = torch.arange(0,12,dtype=torch.float32).view(2,3,2)\n",
    "x.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5000,  2.5000,  4.5000],\n",
       "        [ 6.5000,  8.5000, 10.5000]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean over last dimension, at first, x's dim is 2*3*2, after this step, dim becomes 2*3*1-squeeze-2*3\n",
    "# x.mean(-1)=x.mean(2)\n",
    "x.mean(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# View & Reshape\n",
    "view ans reshape share API<br>\n",
    "view does not allocate new memory<br>\n",
    "reshape works with non-contiguous(不连续的) tensors, but can copy memory<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3],\n",
       "         [ 4,  5]],\n",
       "\n",
       "        [[ 6,  7],\n",
       "         [ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector: [0,1,2,...,1]\n",
    "x=torch.arange(0,12)\n",
    "# viewed as a [2*3*2] tensor\n",
    "x.view(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view as a [3*4] matrix\n",
    "x.view(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1],\n",
       "         [ 2,  3]],\n",
       "\n",
       "        [[ 4,  5],\n",
       "         [ 6,  7]],\n",
       "\n",
       "        [[ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewed as a [3*2*2] tensor via\n",
    "# inferring one unspecified dimension\n",
    "x.view(-1,2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Squeeze(压缩) & Unsqueeze(解压缩)\n",
    "unsqueeze(i)使得原矩阵维度加1，比如原来是0维(1维)的，变成1维向量(2维矩阵),并且在第i个维度上填上1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4494],\n",
       "          [0.4825]]],\n",
       "\n",
       "\n",
       "        [[[0.0557],\n",
       "          [0.7309]]],\n",
       "\n",
       "\n",
       "        [[[0.8156],\n",
       "          [0.0822]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(3,1,2,1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4494, 0.4825],\n",
       "        [0.0557, 0.7309],\n",
       "        [0.8156, 0.0822]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze function can remove all singleton dimensions\n",
    "x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1904, 0.9238, 0.8055])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.rand(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1904, 0.9238, 0.8055]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze fucntion can expand it to 1*3 matrix / row-vector\n",
    "x.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1904, 0.9238, 0.8055]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze in some case equiv.: expand using None indexing\n",
    "y=x[None,:]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equiv.: using view(here -1 can be infer by python, can is indeed 1)\n",
    "z=x.view(-1,3)\n",
    "y.allclose(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x=torch.arange(0,12).view(2,3,2)\n",
    "y=torch.rand(6,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x.view(y.size())=x.view_as(y)\n",
    "x.view_as(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1],\n",
       "        [ 2,  3],\n",
       "        [ 4,  5],\n",
       "        [ 6,  7],\n",
       "        [ 8,  9],\n",
       "        [10, 11]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Expand & Repeat\n",
    "expand and reshape share API<br>\n",
    "reshape allocates new memory<br>\n",
    "expand does not<br>\n",
    "if possible, use expand but watch out for side effects<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [2]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,3)\n",
    "x=x.unsqueeze(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#returns an expanded view of x, -1 is refered to be 3\n",
    "x=x.expand(-1,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7, 7, 7, 7],\n",
       "        [1, 1, 1, 1],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改一个expand后的矩阵的数字，相当于改一个expand之前的数字\n",
    "x[0,1]=7\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,3)\n",
    "x=x.unsqueeze(1)\n",
    "x=x.repeat(1,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 7, 0, 0],\n",
       "        [1, 1, 1, 1],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改一个repeat后的矩阵的数字,不相当于改一个repeat之前矩阵的数字\n",
    "x[0,1]=7\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [1, 1, 1],\n",
       "        [2, 2, 2],\n",
       "        [3, 3, 3],\n",
       "        [4, 4, 4],\n",
       "        [5, 5, 5]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,6).unsqueeze(1)\n",
    "y=torch.rand(6,3)\n",
    "x.expand_as(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Indexing & Advanced indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,24).view(2,3,4)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12, 13, 14, 15],\n",
       "        [16, 17, 18, 19],\n",
       "        [20, 21, 22, 23]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 13, 14, 15])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,0,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Advanced indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [16, 17, 18, 19],\n",
       "         [20, 21, 22, 23]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17, 18],\n",
       "        [21, 22]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1:表示下标1到这个这个维度的最后一个，1:-1表示下标1到这个维度的倒数第二个，最后一个到不了\n",
    "x[1,1:,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 0,  1,  2,  3],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [ 4,  5,  6,  7]],\n",
       "\n",
       "        [[12, 13, 14, 15],\n",
       "         [12, 13, 14, 15],\n",
       "         [20, 21, 22, 23],\n",
       "         [16, 17, 18, 19]]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x[:,indices]=x[:,indices,:], 但indices后面的:可以省略\n",
    "#这里表示抽原来第一个维度为0,0,2,1的行排列起来\n",
    "indices =torch.tensor([0,0,2,1])\n",
    "x[:,indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gather\n",
    "gather的两个输入x和index的矩阵大小必须一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![title](2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[1,2],[3,4]])\n",
    "index=torch.tensor([[0,0],[1,0]])\n",
    "# [0,0]->[0,0],[0,1]->[0,0],[1,0]->[1,1],[1,1]->[1,0]\n",
    "torch.gather(x,1,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 2]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([[1,2],[3,4]]) # equiv: y=torch.arange(1,5).view(2,2)\n",
    "index=torch.tensor([[0,0],[1,0]])\n",
    "# [0,0]->[0,0],[0,1]->[0,0],[1,0]->[1,0],[1,1]->[0,1]\n",
    "torch.gather(x,0,index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1],\n",
       "         [2, 3]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 7]]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,8).view(2,2,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 3],\n",
       "         [2, 1]],\n",
       "\n",
       "        [[4, 5],\n",
       "         [6, 5]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index=torch.tensor([\n",
    "    [[0,1],[1,0]],\n",
    "    [[0,0],[1,0]]\n",
    "])\n",
    "# [0,0,0]->[0,0,0],[0,0,1]->[0,1,1],[0,1,0]->[0,1,0],[0,1,1]->[0,0,1]\n",
    "torch.gather(x,1,index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat, Split, Chunk, and Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cat(catenate的缩写,表示连接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,6).view(3,2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 1],\n",
       "        [2, 3, 2, 3],\n",
       "        [4, 5, 4, 5]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim=1表示cat函数将x按列排列起来\n",
    "torch.cat([x,x],dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Split(拆分)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17],\n",
       "        [18, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,24).view(4,6)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  4,  5],\n",
       "        [ 9, 10, 11],\n",
       "        [15, 16, 17],\n",
       "        [21, 22, 23]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim=1表示split函数将x按照一个个列看待，3表示3列为一个新的tensor\n",
    "torch.split(x,3,dim=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Chunk(块)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  3],\n",
       "        [ 8,  9],\n",
       "        [14, 15],\n",
       "        [20, 21]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dim=1表示chunk函数将x按照一个个列看待，2表示分成2个新tensor\n",
    "# 可以看出torch.chunk(x,2,dim=1)[1]=torch.split(x,3,dim=1)[1]\n",
    "torch.chunk(x,3,dim=1)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack(叠)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       "\n",
       "        [[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[ 0,  1,  2],\n",
       "         [ 3,  4,  5]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(0,6).view(2,3)\n",
    "y=torch.arange(6,12).view(2,3)\n",
    "# stack将tensor叠起来,几个叠起来就在最前面乘几,叠起来的东西必须维度一致\n",
    "torch.stack([x,y,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19]])\n",
      "tensor([0, 1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "A=torch.arange(20).view(5,4)\n",
    "print(A)\n",
    "b=torch.arange(4)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "code_folding": [],
    "heading_collapsed": true
   },
   "source": [
    "# Broadcasting\n",
    "Two tensors are “broadcastable” if each tensor has at least one dimension and when iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[-3],\n",
      "        [ 3]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0, -3, -6],\n",
       "        [ 9, 12, 15]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(0,6).view(2,3)\n",
    "y=torch.tensor([[-3],[3]])\n",
    "print(x)\n",
    "print(y)\n",
    "# x and y are broadcastable\n",
    "# 1st trailing dimension: y has size 1\n",
    "# 2nd trailing dimension: both have size 2\n",
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x=torch.empty(3,5,7,9)\n",
    "y=torch.empty(3,5,7,9)\n",
    "# same shape are always broadcastable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.empty((0,))\n",
    "y=torch.empty(2,2)\n",
    "# x and y are not broadcastable\n",
    "# x dose not have at least one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = torch.empty(5,3,4,1)\n",
    "y=torch.empty(    3,1,1)\n",
    "# x and y are broadcastable\n",
    "#这里说的第一个维度,是指最右边那个维度\n",
    "# 1st trailing dimension: both have size 1\n",
    "# 2nd trailing dimension: y has size 1\n",
    "# 3rd trailing dimension: x size ==y size\n",
    "# 4th trailing dimension: y dim doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x=torch.empty(5,2,4,1)\n",
    "y=torch.empty(  3,1,1)\n",
    "# x and y are not broadcastable\n",
    "# in the 3rd trailing dimension 2！=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[2, 4, 7],\n",
      "                       [3, 2, 1]]),\n",
      "       values=tensor([3., 4., 5.]),\n",
      "       size=(10, 100000000), nnz=3, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.0000],\n",
       "        [0.1181],\n",
       "        [0.0000],\n",
       "        [3.1860],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.5532],\n",
       "        [0.0000],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.LongTensor([[2,4,7],[3,2,1]])\n",
    "values=torch.FloatTensor([3,4,5])\n",
    "# sparse, 利用两个tensor生成,其中一个表示值，另一个表示索引\n",
    "x=torch.sparse.FloatTensor(\n",
    "indices, values, torch.Size([10,100000000]))\n",
    "print(x)\n",
    "# dense\n",
    "m = torch.rand(100000000,1)\n",
    "# sparse.mm相当于普通的torch.mm。可以用来求梯度，但是会返回一个sparse的矩阵。\n",
    "torch.sparse.mm(x,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices=torch.LongTensor([[2,4,7],[3,2,1]])\n",
    "values=torch.FloatTensor([3,4,5])\n",
    "# sparse\n",
    "sparse_x=torch.sparse.FloatTensor(\n",
    "indices, values, torch.Size([10,100000000])\n",
    ")\n",
    "# dense\n",
    "dense_x=torch.zeros(10,100000000)\n",
    "dense_x[2,3]=3\n",
    "dense_x[4,2]=4\n",
    "dense_x[7,1]=5\n",
    "# dense\n",
    "m=torch.rand(100000000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这在iPython中被称为线魔术。它们的独特之处在于它们的参数只延伸到当前行的末尾\n",
    "# 并且魔法本身实际上是用于命令行开发的。  timeit用于计算代码的执行时间。\n",
    "# 从下面的代码可以看出，torch.saprse.mm的运算速度要比torch.mm快很多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298 ms ± 7.94 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.mm(dense_x, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.2 µs ± 146 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "torch.sparse.mm(sparse_x,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Einstein Summation Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.arange(6).reshape(2,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3],\n",
       "        [1, 4],\n",
       "        [2, 5]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix transpose\n",
    "torch.einsum('ij->ji',[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sum\n",
    "torch.einsum('ij->',[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 5, 7])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#column sum\n",
    "torch.einsum('ij->j',[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 12])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# row sum\n",
    "torch.einsum('ij->i',[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([0, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 14])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-vector multiplication\n",
    "y=torch.arange(3)\n",
    "print(x)\n",
    "print(y)\n",
    "torch.einsum('ik,k->i',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 25,  28,  31,  34,  37],\n",
       "        [ 70,  82,  94, 106, 118]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix-matrix multiplication\n",
    "x=torch.arange(6).reshape(2,3)\n",
    "y=torch.arange(15).reshape(3,5)\n",
    "torch.einsum('ik,kj->ij',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector dot product\n",
    "x=torch.arange(3)\n",
    "y=torch.arange(3,6)\n",
    "print(x)\n",
    "print(y)\n",
    "torch.einsum('i,i->',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(145)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix dot prodcut\n",
    "x=torch.arange(6).reshape(2,3)\n",
    "y=torch.arange(6,12).reshape(2,3)\n",
    "print(x)\n",
    "print(y)\n",
    "torch.einsum('ij,ij->',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  7, 16],\n",
       "        [27, 40, 55]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hadamard Product\n",
    "x=torch.arange(6).reshape(2,3)\n",
    "y=torch.arange(6,12).reshape(2,3)\n",
    "print(x)\n",
    "print(y)\n",
    "torch.einsum('ij,ij->ij',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n",
      "tensor([3, 4, 5, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0],\n",
       "        [ 3,  4,  5,  6],\n",
       "        [ 6,  8, 10, 12]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector outer product(向量外积)\n",
    "x=torch.arange(3)\n",
    "y=torch.arange(3,7)\n",
    "print(x)\n",
    "print(y)\n",
    "torch.einsum('i,j->ij',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 10, 15])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix diagonal\n",
    "x=torch.arange(0,16).view(4,4)\n",
    "print(x)\n",
    "torch.einsum('ii->i',[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2339,  0.2009, -0.6290],\n",
       "         [-4.6447, -4.2725, -0.0324]],\n",
       "\n",
       "        [[-1.5242,  2.6211,  0.6357],\n",
       "         [-0.4332, -1.0161, -2.9737]],\n",
       "\n",
       "        [[-0.9112,  0.5484, -1.3976],\n",
       "         [-2.1933, -0.7030, -3.6636]]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch matrix-multiplication\n",
    "x=torch.randn(3,2,5)\n",
    "y=torch.randn(3,5,3)\n",
    "#第i层的两个矩阵做普通矩阵乘法\n",
    "torch.einsum('ijk,ikl->ijl',[x,y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 11, 13, 17])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor contraction(张量收缩)\n",
    "x=torch.randn(2,3,5,7)\n",
    "y=torch.randn(11,13,3,17,5)\n",
    "torch.einsum('pqrs,tuqvr->pstuv',[x,y]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9392,  1.3396, -2.0761,  4.5961],\n",
       "        [-4.0851, -3.0293, -2.3530, -3.8772]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bilinear transformation\n",
    "x=torch.randn(2,3)\n",
    "y=torch.randn(4,3,7)\n",
    "z=torch.randn(2,7)\n",
    "#k,l只在左边出现,没在右边出现,所以运算过程中会被求和掉，最后是一个2*4的矩阵\n",
    "torch.einsum('ik,jkl,il->ij',[x,y,z])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# PyTorch Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.6880, grad_fn=<DotBackward>)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variable\n",
    "x = torch.tensor([-1.5, 1.2], requires_grad=True)\n",
    "# constant\n",
    "y = torch.tensor([1.0, -1.3])\n",
    "# variable\n",
    "z =torch.tensor([-2.0, 0.2], requires_grad=True)\n",
    "# output tensor value, but also computation graph\n",
    "x*y@z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.6880, grad_fn=<DotBackward>)\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-2.0000, -0.2600])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([-1.5, 1.2], requires_grad=True)\n",
    "y = torch.tensor([1.0, -1.3])\n",
    "z =torch.tensor([-2.0, 0.2], requires_grad=True)\n",
    "r=x*y@z\n",
    "# 只要计算r的变量有一个是varibale,即requires_grad=True,则r作为一个tensor带有backward()方法\n",
    "print(r)\n",
    "# 在r没有执行backward()方法之前,x.grad没有被赋值\n",
    "print(x.grad)\n",
    "r.backward()\n",
    "x.grad\n",
    "# Torch的一大优势就是可以自动求梯度，甚至有一种观点认为Torch就是带有自动求梯度的numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1063])\n",
      "None\n",
      "tensor([0.1063])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "a= torch.rand(1,requires_grad=True)\n",
    "b=torch.rand(1,requires_grad=True)\n",
    "c=torch.rand(1,requires_grad=True)\n",
    "d=torch.rand(1,requires_grad=True)\n",
    "e=torch.rand(1,requires_grad=True)\n",
    "# compare to: f=c*a*b+d*a*b, 结论:没有区别\n",
    "f=c*a*b+d*a*b\n",
    "f.backward()\n",
    "print(c.grad)\n",
    "print(e.grad)\n",
    "c.grad.zero_() # c.grad.zero_()的解释见下\n",
    "e=a*b\n",
    "f=c*e+d*e\n",
    "f.backward()  \n",
    "print(c.grad)\n",
    "print(e.grad)\n",
    "#注意,e是中间产物,相当于神经网络里面每一层的output,它是不能被求梯度，也是不需要被求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1063])\n",
      "tensor([0.2127])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "a= torch.rand(1,requires_grad=True)\n",
    "b=torch.rand(1,requires_grad=True)\n",
    "c=torch.rand(1,requires_grad=True)\n",
    "d=torch.rand(1,requires_grad=True)\n",
    "f=c*a*b+d*a*b\n",
    "f.backward()\n",
    "print(c.grad)\n",
    "e=a*b\n",
    "f=c*e+d*e\n",
    "f.backward()\n",
    "print(c.grad)\n",
    "# 反向传播的时候,torch自动反向传播,得到的c.grad会被存下来,下次再反向传播,会被叠加上去,所以要进行清零：c.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Autograd Function\n",
    "参考资料:https://blog.csdn.net/tsq292978891/article/details/79364140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MyExpBackward object at 0x12ea93828>\n",
      "y is tensor([1.], grad_fn=<MyExpBackward>)\n",
      "grad_output is tensor([1.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这是课上pdf上关于PyTorch Autograd Function的说明的改进版,函数用的是cos(x)而不用e^x\n",
    "\n",
    "from torch.autograd import Function \n",
    "# Function源码见:https://github.com/0pytorch/pytorch/blob/master/torch/autograd/function.py\n",
    "\n",
    "class MyExp(Function):    # MyExp类继承Function类\n",
    "    @staticmethod         # 装饰器@staticmethod定义的是一种静态方法\n",
    "    def forward(ctx, x):  # x,ctx也可以改成其他变量名，只不过这个ctx和下面backward的ctx占用同一片内存\n",
    "        print(ctx)        # ctx是apply\n",
    "        result = torch.cos(x)  # result的值是下一结点y的值\n",
    "        ctx.save_for_backward(result, x)  # ctx存的值,会传到backward中去,对指数函数,tanh,sigmoid,函数值可以用来求导\n",
    "                                          # 所以对这些函数,存入ctx的往往是result值,当然这里不是这种情况\n",
    "        return result             # 返回的result值存入y中\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # 这里的grad_outputs是存进来的upsteram_gradient\n",
    "        print(\"grad_output is\",grad_output) # upstream_gradient默认值为tensor([1]),这可能与MyExp继承自Function有关\n",
    "        result, x = ctx.saved_tensors  # 注意ctx.saved_tensors是一个list,所以如果左端只有一个result接收值,\n",
    "                                       # 就需要左端写result, 来使result接受list中的元素值\n",
    "        return -torch.sin(x) * grad_output  # 这里的-torch.sin(x)表示的是local_gradient\n",
    "\n",
    "\n",
    "x = torch.tensor([0.], requires_grad=True)\n",
    "y = MyExp.apply(x)  # 这里的apply相当于说启动MyExp中的forward方法,\n",
    "                    # forward的非ctx参数是x,forward方法返回的值存入y中\n",
    "print(\"y is\",y)\n",
    "\n",
    "y.backward()  # 这里的backward相当于说启动MyExp中的backward方法,\n",
    "              # backward的非ctx参数是upstream_gradient\n",
    "              # 如果backward方法未设置参数,则upstream_gradient默认值为1,如果设置了参数,则按照参数来\n",
    "              # 注意,backward方法的返回值不是返回到y,而是返回到x.grad\n",
    "x.grad        # x.grad的返回值是上面backward方法中的返回值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Modules and examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*所有网络成员都应该从nn.Module上继承并且重载forward方法<br>\n",
    "*使用一个module提供了函数性:<br>\n",
    "    *训练变量可追踪<br>\n",
    "    *让你能能轻松在CPU和GPU之间跳转(见.to(device)方法)<br>\n",
    "*要将一个可变tensor注册到一个module的参数中去,你需要用nn.Parameter去封装它<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0095, -0.0135], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear Model Example\n",
    "import torch.nn as nn # nn.Module是所有神经网络单元(neural network modules)的基类\n",
    "class LinearModule(torch.nn.Module): # LinearModule继承了nn.Module\n",
    "    def __init__(self, x_dim, y_dim):\n",
    "        super(LinearModule, self).__init__() # 继承父类的构造函数,可以利用父类的变量了,LinearModule,slef可以省略\n",
    "#         torch.nn.Module.__init__(self)       # super().__init__()相当于torch.nn.Module.__init__(self)\n",
    "        self.W=nn.Parameter(torch.randn(y_dim,x_dim,requires_grad=True)) # \n",
    "        self.b=nn.Parameter(torch.randn(y_dim),requires_grad=True)\n",
    "    def forward(self,x):\n",
    "        return self.W@x+self.b \n",
    "# Some random input and output data\n",
    "# x = torch.randn(5)\n",
    "# y = torch.randn(2)\n",
    "\n",
    "model=LinearModule(5,2)   # model是LinearModule的一个实例,实际上是调用了__init__方法(一种magic方法)\n",
    "\n",
    "for param in model.parameters():  # model.parameters是参数\n",
    "    print(param.size())\n",
    "model(x)                  # 虽然model是一个实例,但这里实际上调用了__call__方法(一种magic方法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function与Module的差异与应用场景\n",
    "参考资料:https://blog.csdn.net/mdjxy63/article/details/79474966<br>\n",
    "Function与Module都可以对pytorch进行自定义拓展，使其满足网络的需求，但这两者还是有十分重要的不同：<br>\n",
    "Function一般只定义一个操作,因为其无法保存参数,因此适用于激活函数、pooling等操作;<br>\n",
    "Module是保存了参数，因此适合于定义一层，如线性层，卷积层，也适用于定义一个网络<br>\n",
    "Function需要定义三个方法：\\_\\_init\\_\\_,forward,backward(需要自己写求导公式);<br>\n",
    "Module：只需定义\\_\\_init\\_\\_和forward,而backward的计算由自动求导机制构成<br>\n",
    "可以不严谨的认为，Module是由一系列Function组成，因此其在forward的过程中，Function和Variable组成了计算图,在backward时,只需调用Function的backward就得到结果,因此Module不需要再定义backward.<br>\n",
    "Module不仅包括了Function,还包括了对应的参数,以及其他函数与变量,这是Function所不具备的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Checking\n",
    "see torch.autograd.gradcheck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如何确定我们的反向传播正确实现了呢？用\"有限差分近似\"<br>\n",
    "二阶中心差商(一维输出情形):\n",
    "$$\n",
    "\\frac { \\partial f ( \\mathbf { x } ) } { \\partial \\mathbf { x } } \\approx \\frac { 1 } { 2 \\epsilon } ( f ( \\mathbf { x } + \\epsilon ) -  f ( \\mathbf { x } - \\epsilon ) )\n",
    "$$\n",
    "多维输出情形:\n",
    "$$\n",
    "\\boldsymbol { d } ^ { \\top } \\nabla f ( \\boldsymbol { x } ) \\approx \\frac { 1 } { 2 \\varepsilon } ( f ( \\boldsymbol { x } + \\varepsilon \\cdot \\boldsymbol { d } ) - f ( \\boldsymbol { x } - \\varepsilon \\cdot \\boldsymbol { d } ) )\n",
    "$$\n",
    "where, $d \\in \\mathbb { R } ^ { n }$ 是任意一个方向的向量,测试的时候,会多次尝试各种向量方向，有时候选择n个标准基向量,不过随机选择方向也足够好.注意一定要用中心差商.<br>\n",
    "为什么不直接用有限差商代替梯度？<br>\n",
    "对低维函数而言,我们可以用有限差商代替梯度.它也足够精确.所以我们不选用有限差商的原因不是因为它足够精确,而是它足够高效,因为有限差商的时间复杂度是自动求梯度的时间复杂度的n倍.\n",
    "详细讨论见:https://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \\_\\_init\\_\\_方法\n",
    "参考资料:<br>\n",
    "https://blog.csdn.net/hellocsz/article/details/82795514<br>\n",
    "https://www.cnblogs.com/insane-Mr-Li/p/9758776.html<br>\n",
    "既然__init__方法也是类的一个方法,那用和不用__init__方法有什么区别呢?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "12\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "# 不用__init__方法定义类\n",
    "class Rectangle():\n",
    "    def getPeri(self, a, b):\n",
    "\n",
    "        return (a + b) * 2\n",
    "\n",
    "    def getArea(self, a, b):\n",
    "\n",
    "        return a * b\n",
    "\n",
    "\n",
    "rect = Rectangle()\n",
    "\n",
    "print(rect.getPeri(3, 4))\n",
    "\n",
    "print(rect.getArea(3, 4))\n",
    "\n",
    "print(rect.__dict__)  # 可以用__dict__查看实例的属性\n",
    "\n",
    "#可以看出,不用__init__方法定义的类构造的实例是没有属性的,这个实例只象征能方法的集合."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "12\n",
      "{'a': 3, 'b': 4}\n"
     ]
    }
   ],
   "source": [
    "# 用__init__方法定义类\n",
    "class Rectangle():\n",
    "    def __init__(self, a, b):\n",
    "\n",
    "        self.a = a\n",
    "\n",
    "        self.b = b\n",
    "\n",
    "    def getPeri(self):\n",
    "\n",
    "        return (self.a + self.b) * 2\n",
    "\n",
    "    def getArea(self):\n",
    "\n",
    "        return self.a * self.b\n",
    "\n",
    "\n",
    "rect = Rectangle(3, 4)\n",
    "\n",
    "print(rect.getPeri())\n",
    "\n",
    "print(rect.getArea())\n",
    "\n",
    "print(rect.__dict__)\n",
    "\n",
    "#可以看出,不用__init__方法定义的类构造的实例是有属性的,而且可以对自身属性做运算."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python中的\\*args and **Kargs\n",
    "在方法的参数个数不固定的情况下,我们需要让我们的方法去调用可变长的参数(这样的方法泛用性好),这时就需要引入\\*args和**Kargs<br>\n",
    "参考资料:<br>\n",
    "https://www.cnblogs.com/chaojiyingxiong/p/9223754.html<br>\n",
    "https://www.cnblogs.com/yunguoxiaoqiao/p/7626992.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# python中规定参数前带*的,称为可变位置参数,一般用*args来表示\n",
    "# *args用来将参数打包成tuple给函数体(方法)调用\n",
    "def Jiafa(*args):\n",
    "    sum = 0\n",
    "    for i in args:\n",
    "        sum = sum + i\n",
    "    print(sum)\n",
    "Jiafa(1, 3, 5)\n",
    "Jiafa(2, 4, 6, 8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3}\n",
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n"
     ]
    }
   ],
   "source": [
    "# python中规定参数前带**的,称为可变关键字参数,一般用**Kargs来表示\n",
    "# **kwargs 打包关键字参数成dict给函数体(方法)调用\n",
    "def zidian(**kwargs):\n",
    "\n",
    "    print(kwargs)\n",
    "\n",
    "zidian(a=1,b=2,c =3)\n",
    "zidian(a=1,b=2,c =3 ,d =4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instance method,class method,static method\n",
    "是否与类或者实例进行绑定，这就是实例方法，类方法，静态方法的区别。(详细讨论见参考资料)<br>\n",
    "参考资料:https://blog.csdn.net/lihao21/article/details/79762681<br>\n",
    "下面是一些例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## instance method(实例方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leo\n",
      "lee\n"
     ]
    }
   ],
   "source": [
    "class Kls(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def printd(self):\n",
    "        print(self.data)\n",
    "\n",
    "\n",
    "ik1 = Kls('leo')\n",
    "ik2 = Kls('lee')\n",
    "\n",
    "ik1.printd()\n",
    "ik2.printd()\n",
    "#调用实例方法的时候,实例会作为参数传入方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class method(类方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class Kls(object):\n",
    "    num_inst = 0\n",
    "\n",
    "    def __init__(self):\n",
    "        Kls.num_inst = Kls.num_inst + 1\n",
    "\n",
    "    @classmethod       # 类方法修饰器\n",
    "    def get_no_of_instance(cls):\n",
    "        return cls.num_inst\n",
    "\n",
    "\n",
    "ik1 = Kls()\n",
    "ik2 = Kls()\n",
    "\n",
    "print(ik1.get_no_of_instance())\n",
    "print(Kls.get_no_of_instance())\n",
    "# 调用实例方法的时候,类会作为参数传入方法,所以可以用类调用类方法,也可以用实例调用类方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## static method(静态方法)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset done for: 24\n",
      "DB connection made for: 24\n"
     ]
    }
   ],
   "source": [
    "IND = 'ON'\n",
    "\n",
    "\n",
    "class Kls(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    @staticmethod\n",
    "    def checkind():\n",
    "        return IND == 'ON'\n",
    "\n",
    "    def do_reset(self):\n",
    "        if self.checkind():\n",
    "            print('Reset done for: %s' % self.data)\n",
    "\n",
    "    def set_db(self):\n",
    "        if self.checkind():\n",
    "            print('DB connection made for: %s' % self.data)\n",
    "\n",
    "\n",
    "ik1 = Kls(24)\n",
    "ik1.do_reset()\n",
    "ik1.set_db()\n",
    "# 调用静态方法,不需要将实例或者类作为参数传入静态方法,所以可以认为静态方法是个纯函数,输入的变量就是你给的那些变量.\n",
    "# 因为静态方法不需要将实例或者类作为参数传入,所以实例和类都可以调用静态方法\n",
    "# 静态方法很像是在实例和类外面定义的函数,只是我们可以通过实例和类来使用这个函数而已"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 类的继承,重写,super函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object类-新式类，经典类\n",
    "*新式类是指继承object类的类<br>\n",
    "*经典类是指没有继承object类的类<br>\n",
    "参考：<br>\n",
    "https://www.cnblogs.com/attitudeY/p/6789370.html<br>\n",
    "http://www.runoob.com/note/28629<br>\n",
    "为什么要引入新式类呢？(因为使用经典类在多继承问题上会有bug),具体地:\n",
    "![title](3.png)\n",
    "BC 为A的子类,D为BC的子类,A中有save方法,C对其进行了重写<br>\n",
    "在经典类中 调用D的save方法 搜索按深度优先 路径B-A-C， 执行的为A中save 显然不合理<br>\n",
    "在新式类的 调用D的save方法 搜索按广度优先 路径B-C-A， 执行的为C中save<br>\n",
    "但是,事实上,上面的问题只出现在python2.7中<br>\n",
    "在python3中,新式类已经兼容了经典类了,即无论你是否继承object类,都是使用广度优先搜索,执行的是C中的save<br>\n",
    "下面见实例:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is D\n",
      "Come from C\n"
     ]
    }
   ],
   "source": [
    "# 实例\n",
    "# 经典类\n",
    "class A:\n",
    "    def __init__(self):\n",
    "        print('this is A')\n",
    "    def save(self):\n",
    "        print('Come from A')\n",
    "\n",
    "class B(A):    # B继承类A,并且没有重写save方法\n",
    "    def __init__(self):\n",
    "        print('this is B')\n",
    "\n",
    "class C(A):    # C继承类A,并且重写save方法\n",
    "    def __init__(self):\n",
    "        print('this is C')\n",
    "    def save(self):\n",
    "        print('Come from C')\n",
    "        \n",
    "class D(B,C):\n",
    "    def __init__(self):\n",
    "        print('this is D')\n",
    "d1=D()      # 用类D初始化一个对象d1\n",
    "d1.save()   # 若是python2.7的版本,这时候输出的是Come from A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is D\n",
      "come from C\n"
     ]
    }
   ],
   "source": [
    "#新式类\n",
    "class A(object):\n",
    "    def __init__(self):\n",
    "        print ('this is A')\n",
    "\n",
    "    def save(self):\n",
    "        print ('come from A')\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self):\n",
    "        print ('this is B')\n",
    "\n",
    "class C(A):\n",
    "    def __init__(self):\n",
    "        print ('this is C')\n",
    "    def save(self):\n",
    "        print ('come from C')\n",
    "\n",
    "class D(B,C):\n",
    "    def __init__(self):\n",
    "        print ('this is D')\n",
    "\n",
    "d1=D()\n",
    "d1.save()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类的继承,类的重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent\n",
      "Child\n",
      "HelloWorld from Parent\n",
      "Child bar fuction\n",
      "I'm the parent.\n"
     ]
    }
   ],
   "source": [
    "# 这个例子既包含类的普通继承,类的结构函数继承,又包含类的重写\n",
    "class FooParent(object):        # FooParent(父类),object可以省略\n",
    "    def __init__(self):\n",
    "        self.parent = 'I\\'m the parent.'\n",
    "        print ('Parent')\n",
    "    \n",
    "    def bar(self,message):\n",
    "        print (\"%s from Parent\" % message)\n",
    " \n",
    "class FooChild(FooParent):     # FooChild(子类)继承FooParent(父类)\n",
    "    def __init__(self):        # self是当前类的实例,为了方便在类定义中调用其自己类对应的值和函数,详见知乎专栏\n",
    "        super(FooChild,self).__init__() # 调用父类,第一个括号的参数中的参数可以省略(python3版本下)\n",
    "#         FooParent.__init__(self)     # 这行代码可以替换super(FooChild,self).__init__(),功能完全一样\n",
    "        print ('Child')\n",
    "        \n",
    "    def bar(self,message):      #\n",
    "        super().bar(message)            # 省略了第一个括号下的参数\n",
    "        print ('Child bar fuction')\n",
    "        print (self.parent)\n",
    " \n",
    "if __name__ == '__main__':  # 只在运行本程序是调用下面的内容,在被当成库函数导入其他地方时不调用下面内容,详见知乎专栏-pyhton学习\n",
    "    fooChild = FooChild()   \n",
    "    fooChild.bar('HelloWorld')\n",
    "#生成结果的过程:fooChild对象被初始化,由super所以调用了父类初始化函数,为self.parent赋值\"I'm the parent\"\n",
    "#打印了Parent,接下来调用子类初始化函数打印了Child,接着fooChild传入参数HelloWorld并调用子类bar函数\n",
    "#又由super所以传入HelloWorld调用了父类bar函数,打印了”HelloWorld from Parent“,继续往下,打印了“Child bar funciton”\n",
    "#最后打印self.parent\"I'm the parent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child\n",
      "HelloWorld from Parent\n",
      "Child bar fuction\n"
     ]
    }
   ],
   "source": [
    "# 这个例子只包含类的普通继承\n",
    "class FooParent(object):        # FooParent(父类)\n",
    "    def __init__(self):\n",
    "        self.parent = 'I\\'m the parent.' # 父类参数初始化\n",
    "        print ('Parent')\n",
    "    \n",
    "    def parent_bar(self,message):  # 父类方法\n",
    "        print (\"%s from Parent\" % message)\n",
    " \n",
    "class FooChild(FooParent):     # FooChild(子类)继承FooParent(父类)\n",
    "    def __init__(self):        # 专业术语:构造函数(初始化方法)\n",
    "#         super(FooChild,self).__init__() # 只能通过super调用父类初始化方法\n",
    "#         print (self.parent)   # 若没有申明调用父类初始化方法,则无法使用父类变量,即继承并没有将父类变量继承过来   \n",
    "        print ('Child')\n",
    "        \n",
    "    def child_bar(self,message):      # 子类方法\n",
    "        print ('Child bar fuction')\n",
    " \n",
    "if __name__ == '__main__':  \n",
    "    fooChild = FooChild()\n",
    "    fooChild.parent_bar('HelloWorld')\n",
    "    fooChild.child_bar('HelloWorld')\n",
    "# 注意这里和上面不一样的一点是,上面定义和父类方法和子类方法是同名的,这时候,python默认先去调用子类的那个同名方法,\n",
    "# 而父类的那个同名方法则无效,这就是类的重构,所以这时只能通过super来调用父类方法\n",
    "# 而这是这两个方法通过直接继承的途径，直接调用父类.\n",
    "# 虽然父类方法可以通过继承的方式被调用,但父类的初始化参数没有因为继承而直接被调用(原因是父类的初始化方法__init__根本没有启动)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Loss Functions:$\\quad$ $L(f_{\\theta}(x),y)$\n",
    "* Least squares[nn.MSELoss]\n",
    "$$\\frac{1}{n}(f_{\\theta}(x)-y)^2$$\n",
    "$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
    "    l_n = \\left( x_n - y_n \\right)^2$$\n",
    "$$  \\ell(x, y) =\n",
    "    \\begin{cases}\n",
    "        \\operatorname{mean}(L), & \\text{if}\\; \\text{reduction} = \\text{True},\\\\\n",
    "        \\operatorname{sum}(L),  & \\text{if}\\; \\text{reduction} = \\text{False}.\n",
    "    \\end{cases} $$\n",
    "* Logistic[nn.SoftMarginLoss]\n",
    "$$log(1+exp(-yf_{\\theta}(x))) $$\n",
    "$$\\text{loss}(x, y) = \\sum_i \\frac{\\log(1 + \\exp(-y[i]*x[i]))}{\\text{x.nelement}()}$$\n",
    "* Hinge loss[nn.MultiMarginLoss/nn.MultiLabelMarginLoss]\n",
    "$$max(0,1-yf_{\\theta}(x)) $$\n",
    "$$\\text{loss}(x, y) = \\sum_{ij}\\frac{\\max(0, 1 - (x[y[j]] - x[i]))}{\\text{x.size}(0)}$$\n",
    "* Cross-entropy[nn.CrossEntropyLoss]\n",
    "$$-[ylog(f_{\\theta}(x))-(1-y)log(1-f_{\\theta}(x))]   $$\n",
    "$$\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "                   = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)$$\n",
    "$$\\text{loss}(x, class) = weight[class] \\left(-x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\\right)$$\n",
    "* ……and many more: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3955, grad_fn=<AddBackward0>)\n",
      "tensor(2.3955, grad_fn=<MseLossBackward>)\n",
      "tensor(35.9326, grad_fn=<SumBackward0>)\n",
      "tensor(35.9326, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Least squares[nn.MSELoss]\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# reduction=\"mean\"\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "loss=nn.MSELoss(reduction='mean')    \n",
    "print(sum(sum((input-target)*(input-target)/15)))\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "\n",
    "# reduction=\"sum\"\n",
    "loss=nn.MSELoss(reduction='sum')    \n",
    "print(torch.sum((input-target)*(input-target)))\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7085, grad_fn=<DivBackward0>)\n",
      "tensor(0.7085, grad_fn=<SoftMarginLossBackward>)\n",
      "tensor(10.6279, grad_fn=<SumBackward0>)\n",
      "tensor(10.6279, grad_fn=<SoftMarginLossBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1219,  0.2224,  0.2594,  1.1426,  0.0599],\n",
       "        [-2.3126,  0.1061, -0.5628,  0.2709, -0.1776],\n",
       "        [ 0.1891, -0.7172, -1.5706, -0.2456,  0.5956]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Logistic[nn.SoftMarginLoss]\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# reduction=\"mean\"\n",
    "\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n",
    "loss=nn.SoftMarginLoss(reduction='mean')\n",
    "# print(sum(sum((input-target)*(input-target)/15)))\n",
    "print(sum(sum(torch.log(1+torch.exp(-input*target))))/15)  # sum默认是求和一个维度\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()\n",
    "\n",
    "input.grad.zero_()    # 不进行这个步骤,后面得到的input.grad会叠加上这里得到的\n",
    "\n",
    "# reduction=\"sum\"\n",
    "\n",
    "loss=nn.SoftMarginLoss(reduction='sum')\n",
    "print(torch.sum(torch.log(1+torch.exp(-input*target))))   # torch.sum默认是求和所有\n",
    "output = loss(input, target)\n",
    "print(output)\n",
    "output.backward()\n",
    "\n",
    "input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 1, 0, 0],\n",
      "        [1, 0, 1, 1, 1],\n",
      "        [1, 0, 0, 1, 1]])\n",
      "tensor(2.5594, grad_fn=<MultilabelMarginLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Hinge loss[nn.MultiMarginLoss/nn.MultiLabelMarginLoss]\n",
    "# reduction属性也有mean和sum,和上面类似,这里不再重复写了\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randn(3, 5)   这是分类问题,不应该用随机生成float类型,而应该用randint\n",
    "target=torch.randint(0,2,(3,5))\n",
    "print(target)\n",
    "loss=nn.MultiLabelMarginLoss()\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 4])\n",
      "tensor(1.7507, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Cross-entropy[nn.CrossEntropyLoss]\n",
    "# reduction属性也有mean和sum,和上面类似,这里不再重复写了\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# target = torch.randn(3, 5)   这是分类问题,不应该用随机生成float类型,而应该用randint\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "print(target)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "output = loss(input, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7653, 0.2033, 0.4984, 0.2717],\n",
      "        [0.3400, 0.4109, 0.2760, 0.0834],\n",
      "        [0.6182, 0.2758, 0.8907, 0.3001]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.7235, 0.8901, 1.6652, 0.6551])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=torch.rand(3,4)\n",
    "print(a)\n",
    "a.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "228px",
    "left": "1130px",
    "right": "20px",
    "top": "44px",
    "width": "536px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
